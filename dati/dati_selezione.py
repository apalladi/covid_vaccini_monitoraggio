# -*- coding: utf-8 -*-
'''dati_selezione.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PZFyKTVSr39kJENT-IWRMIWq7MZR4Q7_

Extraction of table 3from ISS weekly covid-19 reports
https://www.epicentro.iss.it/coronavirus/sars-cov-2-sorveglianza-dati

See example pdf:
https://www.epicentro.iss.it/coronavirus/bollettino/Bollettino-sorveglianza-integrata-COVID-19_8-settembre-2021.pdf

Requirements: Java 8+, Python 3.6+
'''


# To use notebook on Google Colab
# %%capture
# !pip install tabula-py


from subprocess import CalledProcessError
from tabula import read_pdf
import pandas as pd
import numpy as np
from datetime import date
from datetime import timedelta


def get_pdf_url(weeks_ago):
    '''get_pdf_url(int) -> string, datetime

    weeks_ago: number of weeks to look at since current date
    return: url and date of the found report'''

    # Define variables
    rep_url = "https://www.epicentro.iss.it/coronavirus/bollettino/"
    rep_url += "Bollettino-sorveglianza-integrata-COVID-19_"
    month_num_to_string = {"1": "Gennaio",
                           "2": "Febbraio",
                           "3": "Marzo",
                           "4": "Aprile",
                           "5": "Maggio",
                           "6": "Giugno",
                           "7": "Luglio",
                           "8": "Agosto",
                           "9": "Settembre",
                           "10": "Ottobre",
                           "11": "Novembre",
                           "12": "Dicembre"
                           }
    # Get url
    today = date.today()
    offset = (today.weekday() - 2) % 7 + 7*weeks_ago
    last_wednesday = today - timedelta(days=offset)
    link_day = last_wednesday.day
    link_month = month_num_to_string[str(last_wednesday.month)]
    link_year = last_wednesday.year
    rep_url += f"{link_day}-{link_month}-{link_year}.pdf"
    rep_date = pd.to_datetime(f"{link_day}/{last_wednesday.month}/{link_year}")
    return rep_url, rep_date


def date_parser(x):
    '''date_parser(object) -> datetime

    x: dataframe object
    return: converts argument to datetime'''

    return pd.to_datetime(x, format="%Y/%m/%d")


'''Select mode:
- Automatic (auto = True): table 3 of last available PDF is automatically read
- Manual: you have to specify PDF link and report date
'''

# Define mode
auto = True

# Replace with index of table of interest
table_index = 2

if auto:
    # Compose URL of last pdf
    url_, rep_date = get_pdf_url(0)
    try:
        # Read table from the returned url
        raw_tb = read_pdf(url_, pages="all", stream=True, silent=True)
    except CalledProcessError:
        # PDF of last wednesday has not been published yet
        # Get PDf of 1 weeks ago
        url_, rep_date = get_pdf_url(1)
        raw_tb = read_pdf(url_, pages="all", stream=True, silent=True)
else:
    # Replace with pdf url
    url_ = "https://www.epicentro.iss.it/coronavirus/bollettino/"
    url_ += "Bollettino-sorveglianza-integrata-COVID-19_22-settembre-2021.pdf"
    # Replace with report data
    rep_date = pd.to_datetime("22/09/2021")
    # Read all tables
    raw_tb = read_pdf(url_, pages="all", stream=True, silent=True)

# keep the last and the third last column
columns_to_keep = raw_tb[table_index].columns[[-3, -1]]
to_exclude = r"\((.*)|[^a-z-0-9]|\d+-\d+|\d+\+"

df = raw_tb[table_index][columns_to_keep]
df = df.replace(to_exclude, "", regex=True).replace("", np.nan)
df = df.dropna(subset=columns_to_keep, how="all").fillna(0).astype(np.int64)
df.columns = ["Non vaccinati", "Immunizzati"]


# get data
# sum value by age/event

step_ = 4  # groups (=5) are 4 rows (=20) distant (see foo.pdf)

results = [df[col][i:i+step_].sum()
           for i in np.arange(0, len(df)-step_+1, step_) for col in df.columns]


''' # Create results_df to merge results (#DEBUGONLY)
no_vax = pd.Series(results[::2])
yes_vax = pd.Series(results[1::2])
results_df = pd.concat([no_vax, yes_vax], axis=1)
results_df.columns = ["Non vaccinati", "Immunizzati"]
results_df.index = ["Popolazione",
                    "Diagnosi Sars-CoV-2",
                    "Ospedalizzazioni",
                    "Ricoveri in Terapia Intensiva",
                    "Decessi"] '''


# read the original general data csv from apalladi"s repo
# https://github.com/apalladi/covid_vaccini_monitoraggio/tree/main/dati

repo_url = "https://raw.githubusercontent.com/apalladi/covid_"
repo_url += "vaccini_monitoraggio/main/dati/dati_ISS_complessivi.csv"
df_0 = pd.read_csv(repo_url,
                   sep=";",
                   parse_dates=["data"],
                   date_parser=date_parser,
                   index_col="data"
                   )

# add the new row at the top of the df
df_0.loc[rep_date] = results
df_0.sort_index(ascending=False, inplace=True)

# save to a csv
df_0.to_csv("dati_ISS_complessivi.csv", sep=";")

# get data by age
ages = ["12-39", "40-59", "60-79", "80+"]
results_ = {age: df[ages.index(age)::step_].stack().values for age in ages}

# load dict as df
df_1 = pd.DataFrame(results_).T
df_1.columns = df_0.columns
df_1.index.rename("età", inplace=True)

# save to csv
df_1.to_csv(f"data_iss_età_{rep_date.date()}.csv", sep=";")
